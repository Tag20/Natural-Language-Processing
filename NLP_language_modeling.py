# -*- coding: utf-8 -*-
"""NLPExp3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vjEY1OFL8Kt2KhS3TuCIpH3DJWERNQ6z

# **Aim: Implement N-Gram language model using SpaCy and NLTK+**
"""

!pip install spacy-ngram

"""# **Question 1 :**
### Implement n-gram model.
"""

import spacy
import nltk
from nltk import ngrams
from spacy_ngram import NgramComponent

"""### **NLTK**"""

sentence = "a quick white rabbit jumps over a furious, lazy cat"
for i in range (1,4):
  print(f"\nN-gram: {i}")
  n_grams = ngrams(sentence.split(), i)
  for grams in n_grams:
    print(grams)

sentence = input("Enter the sentence: ")
n = int(input("Enter the value of n: "))
n_grams = ngrams(sentence.split(), n)
for grams in n_grams:
    print(grams)

"""### **SPACY**"""

nlp = spacy.load('en_core_web_sm')

def generate_ngrams(text, n):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    ngrams = [tuple(doc[i:i+n]) for i in range(len(doc) - n + 1)]
    return ngrams

input_text = "a quick brown fox jumps over a lazy dog"
ngrams_result = generate_ngrams(input_text, 1)
print("1-grams:", ngrams_result)
ngrams_result = generate_ngrams(input_text, 2)
print("2-grams:", ngrams_result)
ngrams_result = generate_ngrams(input_text, 3)
print("3-grams:", ngrams_result)

sentence = input("Enter the sentence: ")
n = int(input("Enter the value of n: "))
ngrams_result = generate_ngrams(sentence, n)
print("N-grams user input:", ngrams_result)

"""##**Text Blob**##"""

from textblob import TextBlob

nltk.download('punkt')

nltk.download('reuters')

def n_grams(data, num):
  n_grams = TextBlob(data).ngrams(num)
  return [' '.join(grams) for grams in n_grams]

data = "a quick brown fox jumps over a lazy dog"
print("1-gram: ",n_grams(data,1))
print("2-gram: ",n_grams(data,2))
print("3-gram: ",n_grams(data,3))

"""# **Question 2:**

### Build a basic language model using trigrams of the Reuters corpus.
### Reuters corpus is a collection of 10,788 news documents totaling 1.3 million words. We can build a language model in a few lines of code
"""

from nltk.corpus import reuters
from nltk import bigrams,trigrams
from collections import Counter, defaultdict
model = defaultdict(lambda:defaultdict(lambda:0))
for sentence in reuters.sents():
  for w1,w2,w3 in trigrams(sentence, pad_right=True, pad_left=True):
    model[(w1,w2)][w3] +=1

for w1_w2 in model:
  total_count = float(sum(model[w1_w2].values()))
  for w3 in model [w1,w2]:
    model[w1_w2][w3] /= total_count

reuters_sent = reuters.sents()[1500]

nltk.download('stopwords')
stop_words = set(nltk.corpus.stopwords.words('english'))
data = "a quick brown fox jumps over a lazy dog"
data = [word for word in reuters_sent if word not in stop_words]
print(data)

tri_grams = trigrams(data)

for word in tri_grams:
  print(word)
  tup = tuple(list(word)[:2])
  print(dict(model[tup]))
  print()

"""# **B.1	Observations and learning:**
I learnt how to find the uni-grams, bi-grams, tri-grams and n-grams using NLTK, SPACY and TextBlob. I Learnt how to predict the third word of the tri gram tuple and predict its probabilities as well.

# **B.2	Conclusion:**
From the learnings gained in experiment I will be able to find the uni-grams, bi-grams, tri-grams and n-grams using various NLP libraries.
"""