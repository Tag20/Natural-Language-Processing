# -*- coding: utf-8 -*-
"""NLP Exp2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lM2h5soHlZqywbhxWQ4ZXzlpbUk7zRfI

# **Question 1: Steps for Data Pre-Processing**

Why do we pre process data?



1.   To Measure data quality
2.   Data Cleaning

1.   Data Integration
2.   Data Reduction

1.   Data Transformation

Data preprocessing is crucial for determining the correct input data for machine learning algorithms. Without applying the proper techniques, you can have a worse model result.
Data integration is a crucial step in data pre-processing that involves combining data residing in different sources. It includes multiple databases, data cubes or flat files and works by merging the data from various data sources.
One important purpose of data preprocessing is to sort or sieve all data and to remove and replace outliers with their expected values. The first step in data preprocessing is to detect outliers
"""

!pip install panda

#Importing the required libraries
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('/content/housing_price_dataset.csv')
df

df.describe()

df.isna().sum()

df.dtypes

numeric_features = df.select_dtypes(include=['int64', 'float64'])
categorical_features = df.select_dtypes(include=['object'])
#Select all the columns with data types int64 and float64 and assigns them to the variable
#Similarly second line assignes all objects to the variable categorical_features.

print('There are NULL in numeric_features')
numeric_features.isnull().sum()

print('There are NULL in categorical features')
categorical_features.isnull().sum()

df.isnull().sum()
#Sum of all null values present in df

df.nunique()
#Finds all the unique values in each column

import seaborn as sns
for col in df.select_dtypes(include=[np.number]).columns:
  sns.boxplot(x=col, data=df)
  plt.show()

"""# **Question 2: Techniques for Data Pre-Processing**

Data preprocessing is a crucial step in the data analysis and machine learning pipeline. It involves cleaning, transforming, and organizing raw data into a format suitable for analysis or training machine learning models. Here are some common techniques used in data preprocessing:

Handling Missing Data:

Identify and handle missing values, either by removing rows or columns with missing data or by imputing missing values using techniques like mean, median, or machine learning-based methods.
Data Cleaning:

Correct or remove duplicate records.
Handle outliers by either removing them or transforming them based on the nature of the data.
Data Transformation:

Scaling and Normalization: Standardize or normalize numerical features to a similar scale, e.g., using Min-Max scaling or Z-score normalization.
Log Transformation: Apply log transformations to skewed data to make it more normally distributed.
Binning: Convert numerical features into categorical bins to simplify complex relationships.
One-Hot Encoding: Convert categorical variables into binary vectors.
Handling Categorical Data:

Convert categorical variables into numerical format using techniques like one-hot encoding or label encoding.
Text Data Preprocessing:

Tokenization: Split text into individual words or tokens.
Stopword Removal: Remove common words that do not contribute much to the meaning of the text.
Lemmatization or Stemming: Reduce words to their base or root form.
Handling Special Characters: Remove or handle special characters, punctuation, and numbers.
Feature Engineering:

Create new features based on existing ones to enhance the model's performance.
Extract relevant information from date and time features (e.g., day of the week, month, year).
Handling Imbalanced Data:

Address class imbalance by oversampling the minority class, undersampling the majority class, or using techniques like SMOTE (Synthetic Minority Over-sampling Technique).
Data Splitting:

Split the dataset into training and testing sets to evaluate model performance accurately.
Handling Time Series Data:

Resampling: Adjust the frequency of time series data (e.g., daily to monthly).
Feature Engineering: Create lag features or rolling statistics.
Standardizing Input Data:

Standardize input data for models that are sensitive to the scale of input features.

# **Question 3: What is data normalization**

Data normalization is a technique used in data preprocessing to scale and standardize the features of a dataset. The goal is to transform the data into a common scale without distorting the differences in the ranges of values. This is particularly important when working with machine learning algorithms that are sensitive to the scale of input features, such as gradient-based optimization algorithms (e.g., in neural networks) or distance-based algorithms (e.g., k-nearest neighbors).

Normalization typically involves transforming the values of numerical features to a standard scale, often between 0 and 1. The two common methods for normalization are Min-Max scaling and Z-score normalization.



1.   Min-Max Scaling: This method scales the data to a specific range, usually between 0 and 1.


2.   Z-score Normalization (Standardization):This method standardizes the data by scaling it to have a mean of 0 and a standard deviation of 1.


Choosing between Min-Max scaling and Z-score normalization depends on the nature of the data and the requirements of the specific machine learning algorithm. In general, Min-Max scaling is suitable when the distribution of the data is not necessarily normal, and you want to preserve the original range of values. Z-score normalization is often preferred when the data follows a normal distribution or when the algorithm assumes standardized features.


Normalization is an essential step in the data preprocessing pipeline, ensuring that the features contribute equally to the learning process and preventing issues related to varying scales across different features.

# **Question 4: Libraries nltk and spacy**

**NLTK (Natural Language Toolkit):**

NLTK is a powerful library for natural language processing in Python.
It provides easy-to-use interfaces to perform tasks such as tokenization, stemming, lemmatization, parsing, and more.
NLTK includes various corpora and lexical resources for linguistic research.


**spaCy:**

spaCy is another popular NLP library for Python that is designed for efficiency and production use.
It focuses on providing pre-trained models for various NLP tasks, making it convenient for practical applications.
spaCy is known for its speed and performance, making it suitable for large-scale text processing.
"""

!pip install nltk
!pip install spacy

import nltk
import spacy
from nltk.tokenize import word_tokenize, sent_tokenize

"""# **Question 5: Word and sentence processing in nltk and spacy**"""

nltk.download('punkt')

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize


text = "This is a sample sentence. Tokenize me, please!"
words = word_tokenize(text)
sentences = sent_tokenize(text)

print("NLTK Word Tokenization:", words)
print("NLTK Sentence Tokenization:", sentences)

import spacy

text = "This is a sample sentence. Tokenize me, please!"
nlp = spacy.load("en_core_web_sm")
doc = nlp(text)

spacy_words = [token.text for token in doc]
spacy_sentences = [sent.text for sent in doc.sents]

print("spaCy Word Processing:", spacy_words)
print("spaCy Sentence Processing:", spacy_sentences)

"""# **Question 6: Difference between normal split function and q5 output.**"""

text = "This is a sample sentence. Split me, please!"
split_result = text.split()

print("Split Function Output:", split_result)

"""# **Difference:**

The split function in Python is a basic string method that splits a string into a list of substrings based on a specified delimiter (default is whitespace).
The main difference is that the NLTK and spaCy tokenization methods are more sophisticated. They handle punctuation, contractions, and other linguistic nuances better than a simple split function, which may not handle these cases appropriately.


NLTK and spaCy provide more accurate word and sentence tokenization, especially in the context of natural language processing tasks where linguistic rules are crucial. They tokenize based on a deeper understanding of language structure and semantics, whereas the split function is more basic and may not be suitable for complex linguistic tasks.

# **Question 7: Difference between Stemming & Lemmatization.**

# Stemming vs. Lemmatization:

Stemming:

Output: Produces the root form of a word.
Validity: May result in non-valid words.
Computational Complexity: Faster but less accurate.
Context: Does not consider the context in a sentence.
Lemmatization:

Output: Produces the base or dictionary form of a word.
Validity: Results in valid words, preserving meaning.
Computational Complexity: Slower but more accurate.
Context: Considers the context and applies morphological analysis
"""

import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Download NLTK data (if not already downloaded)
nltk.download('punkt')
nltk.download('wordnet')

# Sample text
text = "Running jumps better than walks. Running and jumping are my favorite activities."

# Tokenize the text
words = word_tokenize(text)

# Stemming using Porter Stemmer
porter_stemmer = PorterStemmer()
stemmed_words = [porter_stemmer.stem(word) for word in words]

print("Stemmed Words:", stemmed_words)

# Lemmatization using WordNet Lemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in words]

print("Lemmatized Words:", lemmatized_words)